\documentclass[a4paper,spanish]{article}


\usepackage[spanish]{babel}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[margin=1.5cm]{geometry}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\newcommand{\oiint}{\displaystyle\bigcirc\!\!\!\!\!\!\!\!\int\!\!\!\!\!\int}


\usepackage{epsfig}
\usepackage{color}
\usepackage{amsfonts}
\usepackage[T1]{fontenc}

\def\Fou {\mathcal{F}}
\def\Rea {\mathcal{R}e}
\def\Ima {\mathcal{I}m}
\def\N {\mathbb{N}}
\def\C {\mathbb{C}}
\def\Q {\mathbb{Q}}
\def\R {\mathbb{R}}
\def\Z {\mathbb{Z}}


%\renewcommand{\contentsname}{\'Indice}
%\renewcommand{\chaptername}{Cap\'\i tulo}
%\renewcommand{\bibname}{Referencias}

\newtheorem{prop}{Proposici\'on}[section]
\newtheorem{teo}[prop]{Teorema}
\newtheorem{defi}[prop]{Definici\'on}
\newtheorem{obs}[prop]{Observaci\'on}
\newtheorem{cor}[prop]{Corolario}
\newtheorem{lema}[prop]{Lema}
\newtheorem{ejem}[prop]{Ejemplo}
\newtheorem{ejer}[prop]{Ejercicio}

\numberwithin{equation}{section}
\newtheorem{definition}{Definicion}


\newenvironment{proof}{
\trivlist \item[\hskip \labelsep\mbox{\it Demostraci\'on:
}]}{\hfill\mbox{$\square$}
%\trivlist \item[\hskip \labelsep{\sl
%#1}\mbox{Demostraci\'on}]}{\hfill\mbox{$\square$}
\endtrivlist}

%\topmargin 0cm \oddsidemargin 0.7cm %% margenes
%\textheight 21cm \textwidth 15cm %% tamaño del texto
\parindent 0cm %% sangria

\begin{document}
\part{\'Analisis de Fourier}

Llegado a este punto esta bueno recalcar las cosas que nos fuimos encontrando en el camino del \'analisis de vibraciones:

\begin{enumerate}
\item Si un n\'umero finito de fuerzas arm\'onicas actúan sobre un sistema linear, la soluci\'on estacionaria del sistema $\psi \left({t}\right)$ resulta en la superposici\'on de las soluciones de cada fuerza particular a su frecuencia $\omega$ respectiva. O sea, vale el principio de superposici\'on.
\item A su vez en el caso de an\'alisis de varios grados de libertad por un $N>1$ tambi\'en vale el principio de superposici\'on sobre los \textit{modos normales} del sistema con frecuencias los autovalores.
\item El movimiento arm\'onico es f\'acilmente extendible a modelizar casos de la vida rutinaria 
\end{enumerate} 

Entonces es claro que el movimiento arm\'onico al cual le dedicamos tanto esfuerzo es un movimiento que vale la pena poder extender a sistemas no lineales. A esto se dedica el presente cap\'itulo, a presentar de la manera mas did\'acticamente posible, pero sin perder el rigor matem\'atico en el camino, el \textbf{\'Analisis de Fourier} (Jean-Baptiste Joseph Fourier $(1768-1830)$, matem\'atico y f\'isico franc\'es conocido por sus trabajos sobre la descomposici\'on de funciones peri\'odicas en series trigonom\'etricas convergentes llamadas Series de Fourier, m\'etodo con el cual consigui\'o resolver la ecuaci\'on del calor. La transformada de Fourier recibe su nombre en su honor) que permite presentar toda funci\'on (con ciertas condiciones leves) como suma infinita o infinitesimal de arm\'onicas. \\
Para esto vamos a repasar primero la estructura y propiedades de los espacios vectoriales con producto interno de dimensi\'on finita y luego extenderemos dimensi\'on infinita para presentar a las series de Fourier como una simple combinaci\'on de los elementos de la base infinita, para luego ver una motivaci\'on del paso de la serie discreta a la continua y luego poder entender la definici\'on de la transformada de Fourier, para finalizar calculando algunas transformadas y algunas propiedades.

\section{Espacios vectoriales con producto interno}
Considerando que lo t\'ipico para el estudiante de F\'isica de la FCEyN es cursar $F2$ con Mate $2$, su \'unico recuerdo de espacios con producto interno es el CBC, por lo que iremos de lo m\'as b\'asico y adelantaremos un poco apurados:
\\
\subsection{Espacios vectoriales y productos internos}
La estructura b\'asica es el \textit{Espacio lineal}a veces denominado Espacio vectorial sobre un \textit{cuerpo de escalares $K$}. Este cuerpo $K$ sera espec\'ificamente en nuestro caso $\R$ o $\C$. Los elementos del espacio vectorial $V$ son llamados \textit{vectores}. Formalmente, un espacio $V \neq \emptyset$ sobre un cuerpo $K$ es un espacio vectorial sii cumple:

\begin{enumerate}
\item \textbf{Adici\'on de vectores}: Existe una operaci\'on llamada suma, generalmente notada como $+$ que satisface \\
\[\forall \ u,v \in V \longrightarrow u+v \in V\]
\item \textbf{Asociatividad}: $\forall u,v,w\in V \longrightarrow \left({u+v}\right)+w = u+\left({v+w}\right)$ \\
\\
$\qquad$ \textbf{Ejercicio interesante:} La l\'inea anterior habla de la $3$-asociatividad de una operaci\'on (o sea asociativa para 3 elementos), probar que para toda operaci\'on $*$ asociada a un conjunto $A \neq \emptyset$ (N\'otese $\left({A,*}\right)$) se tiene $*$ es $3$-asociativa $\leftrightarrow$ $*$ es $n$-asociativa
\item \textbf{Elemento neutro de la suma}: Existe un elemento neutro para la suma, que notaremos $\vec{0}$ y llamaremos \textit{cero} que cumple $\vec{0}+v=v \ \  \forall v \in V$
\item \textbf{Inverso aditivo}: Se tiene $\forall v \in V \ \ \exists ! w \in V \ / v+w=\vec{0}$. A ese $w$ lo llamamos el inverso aditivo o "menos v" de $v$ y lo notamos $\left({- v}\right)$
\item \textbf{Conmutatividad de la suma}: $\forall v,w \in V \ u+v=v+u$
\\
(\textbf{Nota:} Las propiedades antes provistas denotan que $\left({V,+}\right)$ es un \textit{grupo abeliano} (En honor a Niels Henrik Abel $(1802-1829)$ fue un matem\'tico noruego. Es c\'elebre fundamentalmente por haber probado en $1824$ que no hay ninguna f\'rmula para hallar los ceros de todos los polinomios generales de grados  en t\'erminos de sus coeficientes y en el de las funciones el\'ipticas, \'ambito en el que desarroll\'o un m\'etodo general para la construcci\'on de funciones peri\'odicas rec\'iprocas de la integral el\'iptica)

\item \textbf{Multiplicaci\'on por escalares}: $\forall v \in V \ k\in K \ ak\in V$

\textbf{Nota importante:} Por dios notemos que la operaci\'on \textit{producto por escalares} es una aplicaci\'on $ \ast \ : \ {V} \times {K} \rightarrow V$ \textbf{\underline{NO} en $K$!!}

\item \textbf{Distributividad del producto con respecto a la suma}: $ \forall a \in K \ \forall v,w \in V \ a\left({v+w}\right)=av+aw$

\item \textbf{Distributividad de la suma respecto al producto} $\forall a,b \in K \ \forall v \in V \ \left({a+b}\right).v=av+bv $ y $ a\left({bv}\right)=\left({ab}\right)v $

\item \textbf{Elemento neutro para $.$:} $\forall v\in V \ \exists!\overline{a} \in K \ / \ \overline{a}v=v$ a ese elemento de $K$ lo llamamos escalar unitario o \textit{uno} y lo notamos $\overline{a}=1$
\end{enumerate}

Habiendo definido un espacio vectorial, todo subconjunto $W \subseteq V$ que cumpla:
\begin{enumerate}
\item $ \vec{0} \in W $
\item $ av+w \in W \ \forall v,w \in W \ a\in K $
\end{enumerate}

Se le llamar\'a un \textit{subespacio} del espacio vectorial $V$ y tiene la propiedad que \textbf{es un espacio vectorial en s\'i mismo}.\\


\textbf{Definici\'on}: Sea $V$ un $K$ espacio vectorial y $v_1,v_2,\cdots,v_n \in V$ entonces el vector $u$ se dice \textit{combinaci\'on lineal} de los vectores $v_1,v_2,\cdots,v_n $ sii $\exists a_1,a_2,\cdots,a_n \in K$ tal que:
\[u=a_1v_1+a_2v_2+\cdots+a_nv_n\]
Y a la colecci\'on de todos los $u$ que son combinaci\'on lineal de $v_1,v_2,\cdots,v_n \in V$ le llamamos el \textit{espacio generado por los vectores $v_1,v_2,\cdots,v_n$} y se nota $\langle{v_1,v_2,\cdots,v_n}\rangle$ notemos es un subespacio de $V$.\\

\textbf{Definici\'on}: Sea $V$ un $K$ espacio vectorial. Los vectores $v_1,v_2,\cdots,v_n \in V$ se les dice \textit{linealmente independientes} si la ecuaci\'on:
\[a_1v_1+a_2v_2+\cdots+a_nv_n=\vec{0} \ a_1,a_2,\cdots,a_n \in K \]
Vale solo para $a_1=a_2=\cdots=a_n=0$ De otra forma se dice los vectores son \textit{linealmente dependientes}.\\


Se sigue de la definici\'on de independencia lineal que:\\
\[v_1,v_2,\cdots,v_n \ \mbox{son linealmente independientes}\] 
\[\Updownarrow\]
\[\ \forall i\leq n \ v_i \ \mbox{no es combinaci\'on lineal de los otros vectores} \ v_1,v_2,\cdots,v_{i-1},v_{i+1},\cdots,a_n \]

\textbf{Definici\'on}: Un conjunto finito $v_1,v_2,\cdots,v_n \in V$ se les dice una \textit{base} de $V$ si son linealmente independientes y $\langle{v_1,v_2,\cdots,v_n}\rangle = V$. En dicho caso al n\'umero natural $n$ se le llamar\'a \textit{dimensi\'on} de V y se nota $\dim_K{\left(V\right)}=n$.\\
\newline
\textbf{Observaci\'on}: Tanto para dimensi\'on finita como infinita no hay problema de definici\'on de combinaci\'on lineal ya que siempre representa la existencia de una cantidad \textit{finita} de escalares en $K$ para los cuales se cumple la ecuaci\'on.\\

Teniendo estas nociones introduzcamos el producto interno.\\
\newline
\textbf{Definici\'on}: Sea $V$ un $\R$ o $\C$ espacio vectorial. Un \textit{producto interno} en $V$ es una aplicaci\'on $\phi: \ V \times V \rightarrow K \ \ \phi\left({u,v}\right)=\langle{u,v}\rangle$ que cumple:\\
\begin{enumerate}
\item $\forall v,w \in V \ \langle{v,w}\rangle \geq 0$
\item $\forall v\in V \ \langle{v,v}\rangle=0 \longleftrightarrow v=\vec0 $
\item $\forall u,v,w \in V \ \forall a,b \in K \ \langle{au+bv,w}\rangle = a\langle{u,w}\rangle + b\langle{v,w}\rangle$
\item $\forall v,w \in V \ \langle{v,w}\rangle= \overline{\langle{w,v}\rangle}$
\end{enumerate} 

Y a todo espacio vectorial dotado de un producto interno se le llama un \textit{espacio vectorial con producto interno}.\\
Algunas de las propiedades b\'asicas se desprenden de la definici\'on son:

\begin{enumerate}
\item $\forall u,v,w \in V \ \forall a,b \in K \ \langle{u,av+bw}\rangle = \overline{a}\langle{u,v}\rangle + \overline{b}\langle{u,w}\rangle$
\item $\forall u,v \in V \ \forall a\in K \ \langle{au,av}\rangle = \vert{a}\vert ^2 \langle{u,v}\rangle$
\item $\forall v \in V \ \langle{\vec{0},v}\rangle=0 \in K$
\end{enumerate}

\textbf{Ejemplos:}
\begin{enumerate}
\item El espacio eucl\'ideo $V=R^n$ fijamos la base can\'onica $\lbrace{e_1,e_2,\dots,e_n}\rbrace$ tenemos definidos los vectores $\vec{x}=\sum\limits_{i=0}^{n}{x_ie_i}=\left({x_1,x_2,\dots,x_n}\right) \ \vec{y}=\sum\limits_{i=0}^{n}{y_ie_i} = \left({y_1,y_2,\dots,y_n}\right) \ \vec{p}=\left({p_1,p_2,\dots,p_n}\right)$ con el producto interno con el vector de peso $\vec{p}$:
\[\langle{x,y}\rangle_{p}= \sum _{i=0}^{k}{p_i.x_i.y_i}\]
Y en el caso particular de que $p_i=1 \ \forall i\leq n$ tenemos
\[\langle{x,y}\rangle= x_1y_1+x_2y_2+\dots+x_ny_n\]
\textbf{Observaci\'on}: De este simple ejemplo notamos algo vital para el producto interno que resulta ser \emph{vital} fijar primero una base sobre la cual trabajamos.
\item Sea $V=C[a,b]$ el espacio vectorial de las funciones continuas $f: \ [a,b] \rightarrow \C$ con la usual suma y multiplicaci\'on de funciones. Aqu\'i definimos el producto interno para $f,g \in C[a,b]$:
\[\langle{f,g}\rangle = \int_a ^b {f\left({x}\right)\overline{g\left({x}\right)}} dx \]
\textbf{Nota}: Esta espacio vectorial es de dimensi\'on infinita y hallar una base para dicho espacio excede ampliamente los objetivos de hasta un curso de \'algebra lineal, sin embargo el que le interese puede buscarlo en cualquier texto formal de Espacios M\'etricos, aqu\'i simplemente asumiremos que existe y nos basaremos en \'el ciegamente.
\end{enumerate}

\subsection{Espacios normados}

\textbf{Definici\'on}: Sea $V$ un $K$ espacio vectorial. Una \textit{norma} en $V$ se le llama a una aplicaci\'on $\eta: \ V \rightarrow \R_{+}$ que notamos como $\eta \left({v}\right)= \Vert{v}\Vert$ que cumple las siguientes propiedades:
\begin{enumerate}
\item $\forall v \in V \ \Vert{v}\Vert \geq 0$
\item $\Vert{v}\Vert=0 \longleftrightarrow v=\vec{0}$
\item $\forall v \in V \ a\in \C \Vert{av}\Vert=\vert{a}\vert\Vert{v}\Vert$
\item \textbf{Desigualdad triangular}: $\forall u,v,w \in V \Vert{u+v}\Vert\leq\Vert{u}\Vert+\Vert{v}\Vert$
\end{enumerate}

Que justamente nos permiten decir que dados $u,v \in V \ \Vert{u-v}\Vert$ es una \textit{distancia} en $V$ entre $u$ y $v$, de donde $\Vert{v}\Vert$ es la distancia al $\vec{0}$. (\textbf{Aclaraci\'on}: Una distancia es una funci\'on $d:\ V \times V \rightarrow \R_{+}$ que cumple los items $1,2,4$ de antes pero en vez de $3$ solo pide que la funci\'on $d$ sea \textit{sim\'etrica}, es decir, que $d\left(x,y\right)=d\left(y,x\right) \ \forall x,y \in V$).Notemos que a partir de una norma puedo definir una distancia pero \underline{no todas las distancias son normas}\\

\textbf{Desigualdad de Cauchy-Schwartz-Bunyakovski}: Sea $V$ un $K$ espacio vectorial con producto interno, entonces $\forall u,v \in V$ se da:
\[ \vert\langle{v,w}\rangle\vert \leq \Vert{v}\Vert \Vert{w}\Vert \]

De Viktor Yakovlevich Bunyakovsky $(1804-1889)$ fue un matem\'atico ruso que trabaj\'o en mec\'anica te\'orica y la teor\'ia de n\'umeros , y se le atribuye un descubrimiento temprano de la desigualdad de Cauchy-Schwarz, demostrando el caso de dimensi\'on infinita en 1859, muchos a\~nos antes de Hermann Schwarz sobre el tema.

Como observaci\'on \'util es de remarcar que dado un producto interno siempre podemos definir una norma como $\Vert{v}\Vert=\sqrt{\langle{v,v}\rangle}$ usando la desigualdad de Chauchy-Schwarz como herramienta para probar la desigualdad triangular. Pero \underline{no todas las normas provienen de un producto interno}, y tenemos como ejemplo a:

\[\Vert{x}\Vert_\infty = \max \lbrace{\vert{x_i}\vert \ 1\leq i \leq n}\rbrace\]

Que es llamada \textit{norma infinito} o norma uniforme \\

En general si una norma cumple la llamada \textit{regla del paralelogramo} proviene de un producto interno. Es m\'as vale que:

\[\Vert{v}\Vert=\sqrt{\langle{v,v}\rangle} \Longleftrightarrow \Vert{v+w}\Vert ^2 + \Vert{v-w}\Vert ^2 = 2 \Vert{v}\Vert ^2 + 2\Vert{w}\Vert ^2\]

\textbf{Definici\'on}: Sea $V$ un $K$ espacio vectorial con producto interno. Entonces si dados $u,v \in V$ se tiene $\langle{u,v}\rangle=0$ se dice que los vectores $u$ y $v$ son \textit{ortogonales} y lo notaremos $u\perp v$.\\

\textbf{Definici\'on}: Sea $V$ un $K$ espacio vectorial con producto interno. Entonces un conjunto finito $\lbrace{u_k}\rbrace_{k=1} ^n$ o un conjunto infinito $\lbrace{u_k}\rbrace_{k=1} ^{\infty}$ de vectores de $V$ se le dice un \textit{conjunto ortogonal} sii $u_k \neq \vec{0} \ \mbox{y} \ u_i \perp u_k \ \forall i\neq k $. Si adem\'as $\Vert{u_i}\Vert =1 \ \forall i$ se le llama un \textit{conjunto ortonormal}.\\
\newline
Es claro que dado un conjunto ortogonal podemos pasarlo a un conjunto ortonormal denotando $e_k=\cfrac{u_k}{\Vert{u_k}\Vert}$, donde adem\'as se verifica que $\langle{u_1,u_2,\dots,u_n}\rangle = \langle{e_1,e_2,\dots,e_n}\rangle$ y se cumplen las siguientes propiedades:\\

\textbf{Proposici\'on}: Sea $V$ un $K$ espacio vectorial con producto interno, entonces dado un subespacio $W \subseteq V$ generado por el conjunto ortogonal $\lbrace{u_1,u_2,\dots,u_n}\rbrace$ (respectivamente ortonormal) entonces $\lbrace{u_1,u_2,\dots,u_n}\rbrace$ es un conjunto linealmente independiente\\

\textbf{Demostraci\'on}:\\
Sea $\langle{v_1,v_2,\dots,v_n}\rangle=W \subseteq V$ un conjunto ortogonal entonces $\vec{0}=a_1v_1+a_2v_2+\dots+a_nv_n$ y se tiene que
$\forall i\leq n$ \\

\[\begin{array}{rcl}
0 & = & \langle{\vec{0},v_k}\rangle\\
& = & \langle{\sum\limits_{i=1}^{n}{a_iv_i},v_k}\rangle \\
& = & \sum\limits _{i=1}^{n}{a_i\langle{v_i,v_k}\rangle} \\
& = & a_k\langle{v_k,v_k}\rangle \\
& = & a_k\Vert{v_k}\Vert ^2
\end{array}
\]
Como $\Vert{v_k}\Vert ^2 \neq 0 \ \Longrightarrow a_k=0 \ \forall k$

\begin{flushright}
$\square$
\end{flushright}

\textbf{Proposici\'on}: Sea $V$ un $K$ espacio vectorial y sea $\left({e_k}\right)_{i=1}^{n}=\mathcal{B}$ una base ortogonal de $V$, entonces se tiene que:

\[ \forall v \in V \ v=\sum\limits_{i=1}^{n}{a_ie_i} \\
a_i= \cfrac{\langle{v,e_i}\rangle}{\Vert{e_i}\Vert ^2} \]

\textbf{Demostraci\'on}: Sea $v \in V$ entonces:

\[
\begin{array}{rcl}
\langle{v,e_k}\rangle & = & \langle{\sum\limits_{i=1}^{n}{a_ie_i},e_k\rangle} \\
& = & \sum\limits_{i=1}^{n}{a_i\langle{e_i,e_k}\rangle} \\
& = & a_i \Vert{e_k}\Vert \\
& & \\
\Longrightarrow & a_i=\cfrac{\langle{v,e_k}\rangle}{\Vert{e_k}\Vert ^2} &
\end{array}
\]
\begin{flushright}
$\square$
\end{flushright}

Este \'ultimo resultado es muy importante en nuestro estudio ya que en general dado un subesp\'acio $W \subseteq V$ con una base $\mathcal{B}=\left({e_1,e_2,\dots,e_n}\right)$ los coeficientes en una base dada son una combinaci\'on de \textit{todos} los vectores de la base, sin embargo en las bases ortogonales s\'olo dependen cada uno de su respectivo coeficientes.\\

\textbf{Teorema de Pit\'agoras}: Sea $V$ un $K$ espacio vectorial con producto interno, y sea $\left({e_k}\right)_{i=1}^{n}=\mathcal{B}$ una base de $V$. Entonces se ve que:

\[\Vert{\sum\limits_{i=1}^{n}{a_ie_i}}\Vert ^2 = \sum_{i=1}^{n}{\vert{a_i}\vert ^2 \Vert{e_i}\Vert ^2} \]

Cuya demostraci\'on es simple y sigue el esp\'iritu de las dos anteriores. Finalmente cabe preguntarse si dado una base $\mathcal{B}=\left({v_i}\right)_{i=1}^{n}$ de $V$ se puede hallar una base $\mathcal{B'}=\left({e_i}\right)_{i=1}^{n}$ ortogonal (respectivamente ortonormal) tal que $\langle{v_i}\rangle_{i=1}^{k}=\langle{e_i}\rangle_{i=1}^{k}  \ \forall k\leq n$. Y la respuesta es que s\'i y el teorema que lo demuestra es el de \textit{Ortogonalizaci\'on de Gramm-Schmidt} cuya demostraci\'on es simple y aburrida y se deja al lector que la lea de cualquier lado. 

\medskip 

Finalmente antes de adentrarnos en los espacios vectoriales de dimensi\'on infinita analicemos lo siguiente: Sea $V$ un $K$ espacio vectorial con producto interno y sea $\mathcal{B}=\left({v_i}\right)_{i=1}^{k}$ una base ortogonal de $W \subseteq V$, entonces dado $u \notin W \ u \neq \sum\limits_{i=1}^{n}{a_ie_i}$. En ese caso se define a la \textit{proyecci\'on ortogonal} de $u$ sobre $W$ como
\[p_W \left({u}\right) = \sum\limits_{i=1}^{n}{a_ie_i} \]

Que cumple las siguientes propiedades de nuevo aburridas de demostrar:

\begin{enumerate}
\item $\langle{u-p_W\left({u}\right),w}\rangle=0 \  u \in V \ \forall w \in W $
\item $\Vert{u-p_W\left({u}\right)\Vert \leq \Vert{u-w}\Vert \ \forall w\in W}$\\\\
Por lo que la proyecci\'on ortogonal es el vector \textit{m\'as cercano} al subesp\'acio $W$ dado y la distancia se realiza (En general la distancia de un punto a un conjunto se define como un \'infimo y a veces no se realiza en la teor\'ia general de espacios m\'etricos)
\end{enumerate}

\subsection{Sistemas ortonormales infinitos}
Sea $V$ un espacio vectorial con producto interno, asumiremos en adelante que $\dim_K \left(V\right)=\infty$. Sea $\lbrace{e_1,e_2,\dots}\rbrace$ un sistema ortonormal con un n\'umero infinito de vectores, nos encontramos con que el concepto de base es problem\'atico al menos. Entonces por ahora \underline{no} podemos decir que el sistema ortonormal dado es una base de $V$ o siquiera un sistema de generadores. Veamos que tenemos siendo muy cuidadosos:\\

\textbf{Desigualdad de Bessel}: Se tiene $\forall u \in V$ la serie $\sum\limits_{i=1}^{\infty}\vert{\langle{u,e_i}\rangle}\vert^2$
converge y, adem\'as, se tiene que vale

\[\sum\limits_{i=1}^{\infty}\vert{\langle{u,e_i}\rangle}\vert^2 \leq \Vert{u}\Vert ^2\]

(Friedrich Wilhelm Bessel $(1784 - 1846)$ fue un matem\'atico alem\'an, astr\'onomo, y sistematizador de las funciones de Bessel [las cuales, a pesar de su nombre, fueron descubiertas por Daniel Bernoulli])\\

\textbf{Demostraci\'on}
Sea $ S_n = \sum \limits_{i=1}^{n}{\vert{\langle{u,e_i}\rangle}\vert^2} $ Entonces para cada $n\in\N$ la suceci\'on de sumas parciales resulta ser  $\Vert{p_{W_{n}} \left({u}\right)}\Vert ^2$ con $W_n = \langle{e_i}\rangle_{i=1}^{n}$\\
Por Pit\'agoras que no demostramos en dimensi\'on finita (as\'i los obligo a buscarla) sabemos que:

\[ \Vert{u}\Vert^2=\Vert{p_{W_{n}}\left({u}\right)}\Vert^2 + \Vert{u-p_{W_{n}}\left({u}\right)}\Vert^2\]

Por lo que $\Vert{p_{W_{n}}\left({u}\right)}\Vert ^2 \leq \Vert u\Vert ^2$ entonces la sucesi\'on de sumas parciales esta acotada superiormente, y como es mon\'otona creciente se sigue por completitud que converge. Es decir

\[ \lim \limits_{n \rightarrow \infty}{ \sum \limits_{i=1}^{n} {\vert{\langle{u,e_i}\rangle}\vert ^2}} \leq \Vert{u}\Vert^2 \]

\begin{flushright}
$\square$
\end{flushright}

En el caso de darse la igualdad decimos que vale la \textit{Igualdad de Parseval}. Inmediatamente podemos entonces decir que:\\

\textbf{Lema de Riemann-Lebesgue}: Sea $V$ un $K$ espacio vectorial con producto interno y sea $\lbrace{e_1,e_2,\dots}\rbrace$ un sistema ortonormal infinito. Entonces para cada $ u \in V $ se tiene:

\[\lim_{n \rightarrow \infty} {\langle{u,e_n}\rangle} =0 \]

(Georg Friedrich Bernhard Riemann $(1826 - 1866)$ fue un matem\'atico alem\'an que realiz\'o contribuciones muy importantes en an\'alisis y geometr\'ia diferencial, se aburri\'o e invento las sumas, algunas de ellas allanaron el camino para el desarrollo m\'as avanzado de la relatividad general. Su nombre est\'a conectado con la funci\'on zeta, hip\'otesis de Riemann, la integral de Riemann, el lema de Riemann, las variedades de Riemann, las superficies de Riemann y la geometr\'ia de Riemann; en resumen un groso).\\ 

\textbf{Demostraci\'on}: De la desigualdad de Bessel se tiene que la serie converge, lo que necesariamente indica que el argumento tiende a 0, entonces vale el resultado

\begin{flushright}
$\square$
\end{flushright}

Uno de los primeros problemas que nos encontramos al querer definir una base de $V$ es el significado de \textit{combinaci\'on lineal infinita}. Es decir, supongamos $V$ un $K$ espacio vectorial con producto interno y un sistema infinito ortonormal $\lbrace{v_1,v_2,\dots}\rbrace \ v_i \in V \ \forall i \in \N$ y una secuencia de escalares $\lbrace{a_i}\rbrace_{i=1}^{\infty} \ a_i \in K \ \forall i$. A su vez no nos compliquemos mucho y tomemos que los sistemas son secuencias ordenadas (a pesar que en un conjunto no hay orden), entonces podemos dar un significado a la expresi\'on "$\sum \limits _{n=1}^{\infty} {a_nv_n}$" que es una suma infinita de \underline{vectores} y no de escalares!! Es m\'as si le llegamos a dar sentido a esa suma, necesitamos que se mantengan algunas propiedades de las bases, como la unicidad de coeficientes, por ejemplo. Para esto presentamos:\\

\textbf{Definici\'on}: Sea  $\lbrace{v_m}\rbrace_{m=1}^{\infty}$ una suceci\'on infinita de vectores en un espacio vectorial normado $V$. Diremos que la suceci\'on \textit{converge en norma} o en \textit{media cuadr\'atica} a un vector $v \in V$ cuando se tiene que

\[
{Dado} \ \epsilon >0 \ \exists m\left({\epsilon}\right) \ /\forall m\geq m\left({\epsilon}\right) \Longrightarrow \Vert{v-v_m}\Vert < \epsilon \]

Lo que nos deja\\

\textbf{Definici\'on}: Sea $\lbrace{v_1,v_2,\dots}\rbrace$ una suceci\'on infinita de vectores en un espacio vectorial normado $V$ y sea $\lbrace{a_i}\rbrace_{i=1}^{\infty}$ una suceci\'on de escalares. Entonces decimos que la serie $\sum \limits _{n=1}^{\infty} {a_nv_n}$ converge en norma a $v \in V$ y escribimos $v=\sum \limits _{n=1}^{\infty} {a_nv_n}$ cuando se tiene que las sumas parciales $w_m=\sum \limits _{n=1}^{m} {a_nv_n}$ convergen en norma a $v$. Es decir:

\[ v = \sum \limits _{n=1}^{\infty} {a_n v_n} \  \Longleftrightarrow \ \lim \limits _{m \rightarrow \infty}{\Vert{v-w_m}\Vert}=0\]

Y la expresi\'on que $v$ esta generado por $\lbrace{v_1,v_2,\dots}\rbrace$ se extiende a $\exists \lbrace{a_i}\rbrace_{i=1}^{\infty} \ a_i\in K \forall i $ tal que se tenga que $\Vert{v- \sum \limits _{n=1}^{m} {a_nv_n}}\Vert$ tienda a cero cuando $m$ tiende a infinito. \\
Ahora estamos en condiciones de definir LA propiedad que vamos a tener que asumir del sistema trigonom\'etrico al tener que trabajar con las series de Fourier que es la de un sistema cerrado o total:\\

\textbf{Definici\'on}: Sea $\lbrace{v_1,v_2,\dots}\rbrace = W$ un sistema ortonormal infinito en un $K$ espacio vectorial $V$ con producto interno. Diremos que el sistema es \textit{cerrado} o \textit{total} en $V$ si $\forall v \in V$ se tiene que:

\[\lim \limits _{m \rightarrow \infty}{\Vert{v- \sum \limits _{i=1}^{m}{\langle{v,v_i}\rangle v_i}}\Vert}=0\]

Que si recordamos que el vector $\sum \limits _{i=1}^{m}{\langle{v,v_i}\rangle v_i}$ es la proyecci\'on ortogonal de $v$ sobre $W$, por lo que un sistema es cerrado en $V$ si y solo si para todo elemento de $V$ tenemos una combinaci\'on lineal infinita que converge en norma a ese elemento.\\

\textbf{Proposici\'on}: El sistema ortonormal infinito $\lbrace{v_1,v_2,\dots}\rbrace = W$ es cerrado en el $K$ espacio vectorial $V$ con producto interno \textbf{\underline{si y solo si}} $\forall v \in V$ se tiene:

\[\sum\limits_{i=1}^{\infty}\vert{\langle{u,e_i}\rangle}\vert^2 = \Vert{u}\Vert ^2\]

Es decir \textbf{la propiedad de un sistema de ser cerrado en $V$ es equivalente a que $\forall v \in V$ valga la igualdad de Parseval}.\\

\textbf{Proposici\'on}: Para cada m natural, sea $V_m=\langle{e_1,\dots,e_m}\rangle \ y \ u_m=p_{V_m}\left({u}\right)$ sea la proyecci\'on ortogonal de $u$ en $V_m$. Entonces del teorema de Pit\'agoras sabemos que:

\[ \Vert{u}\Vert^2=\Vert{u_m}\Vert^2 + \Vert{u-u_m)}\Vert^2\]

Por lo que se tiene

\[
\Vert{u-u_m)}\Vert^2 = \Vert{u}\Vert^2 - \sum\limits_{i=1}^{\infty}\vert{\langle{u,e_i}\rangle}\vert^2
\]

Por lo que se puede ver que

\[ \lim \limits _{m \rightarrow \infty}{\Vert{u-u_m)}\Vert^2} \ \Longleftrightarrow \ \lim \limits _{m \rightarrow \infty}{\Vert{u}\Vert^2 - \sum\limits_{i=1}^{\infty}\vert{\langle{u,e_i}\rangle}\vert^2
} \]

Que justamente dice que el sistema es cerrado en $V$ sii vale la igualdad de Parseval.

\begin{flushright}
$\square$
\end{flushright}

Otra propiedad importante de los sistemas infinitos es la de \textit{completitud}.\\

\textbf{Definici\'on}: Sea $\lbrace{v_1,v_2,\dots}\rbrace = W$ un sistema ortonormal infinito en un $K$ espacio vectorial $V$ con producto interno. Diremos que el sistema es \textit{completo} si \underline{solo} el vector cero ($\vec{0}=v$) cumple con:

\[\langle{v,v_i}\rangle=0 \ i \in \N\] 

Claramente la propiedad de completitud es copada, y d\'ificil de conseguir, o sea, supongamos un sistema ortonormal infinito y le quitamos un vector, ya ese sistema no es ni completo ni cerrado, pero sigue siendo infinito, lo que implica que hay un factor mas importante que la cardinalidad en este asunto de los infinitos. Es m\'as puede que tengamos un sistema que sea cerrado y completo, pero que el "ritmo de convergencia" a algunos elementos del espacio sea insuficiente y no nos sirve de todos modos.\\
Antes de pasar a la pr\'oxima secci\'on e introducir a las Series de Fourier notemos que claramente un sistema cerrado es completo pero no viceversa, ya que suponiendo es cerrado basta con reemplazar en la igualdad de Parseval $v=0$ y leesto es completo.

\section{Series de Fourier}

En \'esta secci\'on vamos a introducir las Series de Fourier como una representaci\'on o approximaci\'on de ciertas funciones en un intervalo finito. Justamente esto es posible debido a que el sistema trigonom\'etrico es un sistema ortonormal infinito cerrado, como definimos antes, y entonces toda $f$ puede ser aproximada por una combinaci\'on lineal infinita de dichos elementos del sistema.

\subsection{Definiciones}

Sea $E[-\pi,\pi]$ al conjunto de las funciones complejas continuas a trozos. Recordemos una funciones $f: \ [-\pi,\pi] \rightarrow \C$ la llamamos \textit{continua a trozos} si tiene \textbf{a lo sumo} un n\'umero finito de discontinuidades y , adem\'as existen los l\'imites laterales en todo punto. Es decir

\begin{defi}
\[ Dada \ f: \ [-\pi,\pi] \rightarrow \C \quad \exists \lim \limits _{x \rightarrow x_i} {f\left({x}\right)} \ \forall i\leq n \in \N \ y \ sea \ A=\lbrace{x_i \in [-\pi,\pi] \ / \ \lim \limits _{x \rightarrow x_i} \neq f\left({x_i)}\right)}\rbrace \\
\Longrightarrow \ \vert{A}\vert < \infty \]
Entonces decimos $f$ es continua a trozos
\end{defi}

Notemos que $f$ puede no estar definida en las discontinuidades de salto. Es simple ver que $E[-\pi,\pi]$ es un espacio vectorial y si definimos como producto interno

\[ \langle{f,g}\rangle = \cfrac{1}{\pi} \int_{\-pi}^{\pi}{f\left({x}\right).\overline{g\left({x}\right)}} dx\]

Tenemos que $E[-\pi,\pi]$ es un espacio vectorial con producto interno.

\begin{teo}
La secuencia de funciones
\[\lbrace{\cfrac{1}{\sqrt{2}}, \sin{x}, \cos{x}, \sin{2x},\dots}\rbrace\]
es un sistema ortonormal infinito en E
\end{teo}

\subsection{Existencia y unicidad de la serie de Fourier}
Y este sistema es importante por que es cerrado en $E$, el cual vale por el:

\begin{teo}\textbf{Teorema de Stone-Weirstrass}
\\Sea $M$ un espacio m\'etrico compacto y $A\subset \mathcal{C}\left({M;\C}\right)$ un sub\'algebra autoadjunta que contiene las constantes y separa los puntos de $M$. Entonces toda funci\'on continua compleja $f: \ M \rightarrow \C$ puede ser uniformemente aproximada por funciones pertenecientes a $A$ 
\end{teo}

\begin{cor}
Sea $M$ un espacio m\'etrico compacto y $S\subset \mathcal{C}\left({M;\R}\right)$ un conjunto que separa puntos. Entonces toda funci\'on continua $f: \ M \rightarrow \R$ puede ser uniformemente aproximada por polinomios de las funciones de $S$
\end{cor}

Espec\'ificamente podemos tomar los polinomios trigonom\'etricos

\[p\left(t\right)= a_0 + \sum \limits_{k=1}^{n}a_k\cos{kt} + b_k \sin{kt} \]

que constituyen un sub\'algebra de $\mathcal{C}\left({[-\pi,\pi];\R}\right)$ que contiene constante pero que no separa los puntos pues $p(-\pi)=p(\pi)$ para todo polinomio trigonom\'etrico $p$; sin embargo, salvo ese caso en general para $x\neq0 \ y\neq 2\pi \ si \ p(x)=p(y) \Longrightarrow \ x=y$ Por lo que se puede probar que en virtud del corolario mostrado el sub\'algebra de polin\'omios trigonom\'etricos es densa en $\mathcal{C}\left({S^1;\R}\right)$ denotando $S^1$ como la circunferencia unitaria y entonces toda funci\'on continua $f: \ [-\pi,\pi] \rightarrow \R$ (y en general a $\C$) es uniformemente aproximada por los polinomios dados que llamaremos el \textbf{desarrollo de Fourier}.

S\'i, es complicado pero si asumimos lo anterior (que igual val\'ia comentarlo aunque no demostramos el teorema se demuestra en C\'alculo Avanzado) podemos entonces ver que el sistema trigonom\'etrico es cerrado o total en $E$ y entonces anteriormente vimos que si $\lbrace{e_n}\rbrace_{n=1}^{\infty}$ es un sistema ortonormal, podemos representar $f \in E$ como una suma de la forma

\[\sum \limits _{n=1}^{\infty}{\langle{f,e_n}\rangle e_n}\] 

De la cual veamos los coeficientes:

\begin{enumerate}
\item Si $e_n(x)=\cfrac{1}{\sqrt{2}}$ entonces:

\[\langle{f,e_n}\rangle e_n = \cfrac{1}{\pi}\left({ \int_{-\pi}^{\pi}{f(t)\cfrac{1}{\sqrt{2}}}dt }\right)\cfrac{1}{\sqrt{2}} = \cfrac{1}{2\pi} \int_{-\pi}^{\pi}{f(t)}dt\]

\item Si $e_n(x)=\sin{nx} \ n \in \N$ entonces:

\[\langle{f,e_n}\rangle e_n = \cfrac{1}{\pi}\left({ \int_{-\pi}^{\pi}{f(t)\sin{nt}}dt}\right)\sin{nx}\]

\item Si $e_n(x)=\cos{nx} \ n \in \N$ entonces:

\[\langle{f,e_n}\rangle e_n = \cfrac{1}{\pi}\left({ \int_{-\pi}^{\pi}{f(t)\cos{nt}}dt}\right)\cos{nx}\]
\end{enumerate}

Por lo que la serie dada antes tiene la forma de

\begin{equation}
\cfrac{a_0}{2} + \sum \limits_{n=1}^{\infty}a_n\cos{nx} + b_n \sin{nx}
\label{desarrollo_fourier} 
\end{equation}

donde

\begin{equation}
\left\lbrace \begin{array}{rcl} 
a_n & = & \cfrac{1}{\pi} \int_{-\pi}^{\pi}{f(x)\cos{nx}} dx \ n \in \N \\
& & \\
b_n & = & \cfrac{1}{\pi} \int_{-\pi}^{\pi}{f(x)\sin{nx}} dx \ n \in \N
\end{array} \right.
\label{coeficientes_fourier}
\end{equation}

Lo que nos lleva entonces finalmente a notar que la famosa Serie de Fourier no es mas que tomar un sistema ortonormal y representar una combinaci\'on lineal cualquiera de una base dada, no mais!! Veamoslo tranca:

\begin{defi}
Sea $f \in E$ Entonces la serie(\ref{desarrollo_fourier}) associada a $f$, con $a_n \ y \ b_n$ como en (\ref{coeficientes_fourier}) es llamada la \emph{Serie de Fourier} de $f$ y escribimos:
\[f(x) \sim \cfrac{a_0}{2} + \sum \limits_{n=1}^{\infty}a_n\cos{nx} + b_n \sin{nx} \]
\end{defi}

\begin{obs} a\\

\begin{enumerate}

\item La notaci\'on del t\'ermino constante como $\cfrac{a_o}{2}$ es simple notaci\'on est\'andar y luego veremor la utilidad.
\item Notemos en la definici\'on de la serie de Fourier notamos que $f(x) \sim$ a la serie de Fourier, pues no hay necesidad que la serie converga a la funci\'on para alg\'un punto, y menos a\'un a $f(x)$
\item Podemos definir una relaci\'on de equivalencia entre funciones $f,g \in E$ dada por 
\[ f \equiv g \Longleftrightarrow \ f\left({x}\right)=g\left({x}\right) \ \forall x \in [-\pi,\pi] \] salvo para finitos $x_1,x_2,\dots,x_n$
 Y se justifica por la norma del espacio dado que la integral no se modifica por finitos puntos 
\end{enumerate}
\end{obs}

Vale notar que en los casos particulares que $f(x)=f(-x)$, o sea, f sea \textit{par} tenemos que $b_n=0 \ \forall n\in \N$, u el caso alternativo en que $f(x)=-f(-x)$, o sea, f sea \textit{impar} tenemos que $a_n=0 \ n\in\N$. Es m\'as en el caso com\'un de $F2$ en que la extensi\'on de la funci''on sea par o impar pero la funci\'on \textbf{en s\'i} tengo una paridad tenemos que solo los modos pares u impares actuan en el desarrollo de Fourier que habr\'a que analizar en particular. 

\smallskip

No hablaremos aqu\'i mucho m\'as como la deigualdad de Bessel o o el lema de Riemann-Lebesgue la igualdad de Parseval en este caso particular de espacio de Hilbert en este apunte ya que resulta in\'util para nuestro uso; simplemente quer\'ia introducir formalmente a las Series de Fourier, no una exposici\'on completa de sus propiedades y el an\'alisis de los distintos de convergencia (materia que realmente a los f\'isicos les chupa un huevo XD).

\smallskip

Finalmente dada $f \in E[a,b]$ se puede ver que se puede extender a cualquier intervalo finito la serie de Fourier mediante traslaciones y homotecias como

\begin{equation}
f \left({x}\right) \sim \cfrac{a_0}{2} + \sum \limits_{n=1}^{\infty} {a_n \cos{ \cfrac{2n \pi x}{b-a}} + b_n \sin{ \cfrac{2n \pi x}{b-a}}} 
\end{equation}

\begin{equation}
\left\lbrace \begin{array}{rcl} 
a_n & = & \cfrac{2}{b-a} \int_{a}^{b}{f \left({x}\right) \cos{\cfrac{2n \pi x}{b-a}}} dx \ n \in \N \\
& & \\
b_n & = & \cfrac{2}{b-a} \int_{a}^{b}{f \left({x}\right) \sin{\cfrac{2n \pi x}{b-a}}} dx \ n \in \N
\end{array} \right.
\end{equation}

\section{Integral y Transformada de Fourier}

Antes de poder presentar la transformada de Fourier nos conviene hacer un peque\~no repaso de integraci\'on en $\C$ y dos o tres resultados importantes que nos lleven a poder definir correctamente lo que conocemos como Transformada de Fourier.

\subsection{Integraci\'on de funciones de variable real y campo complejo}
Diremos que una funci\'on $\phi : \ [a,b] \rightarrow \C$ es integrable Riemann en el intervalo $[a,b]$, y
escribiremos $\phi \in R([a,b])$, si las funciones $\Rea(\phi)$ y $\Ima(\phi)$ son integrables Riemann en $[a,b]$ (en el sentido que conocemos, ya que son funciones reales de variable real definidas en
un intervalo) en cuyo caso definimos la integral de $\phi$ en $[a,b]$ como el n\'umero complejo

\[ \int_{a}^{b}{\phi(t)} dt = \int_{a}^{b}{\Rea \lbrace\phi(t)\rbrace} dt + i \int_{a}^{b}{\Ima \lbrace\phi(t)\rbrace} dt\]

Es claro que una condici\'on necesaria para la integrabilidad de $\phi$ es la acotaci\'on. La
integrabilidad se puede caracterizar en los siguientes t\'erminos:

\begin{teo}\textbf{Criterio de integrabilidad de Lebesgue}\\
Sea $\phi: \ [a,b] \rightarrow \C$ una funci\'on acotada. Entonces $\phi \in R ([a,b])$ si, y s\'olo si, $\phi$ es continua casi por doquier en $[a,b]$. En particular, toda funci\'on acotada y continua salvo en un n\'umero finito de puntos es integrable.
\end{teo}

(Henri L\'eon Lebesgue $(1875 - 1941)$ fue un matem\'atico franc\'es. Lebesgue es fundamentalmente conocido por sus aportes a la teor\'ia de la medida y de la integral. A partir de trabajos de otros matem\'aticos como \'Emile Borel y Camille Jordan, Lebesgue realiz''o importantes contribuciones a la teor\'ia de la medida en 1901. Al a\~no siguiente, en su disertaci\'on Int\'egrale, longueur, aire (Integral, longitud, \'area) presentada en la Universidad de Nancy, defini\'o la integral de Lebesgue, que generaliza la noci\'on de la integral de Riemann extendiendo el concepto de \'area bajo una curva para incluir funciones discontinuas. Este es uno de los logros del an\'alisis moderno que expande el alcance del an\'alisis de Fourier)

Y habiendo ya definido la integral de una funci\'on de variable real a valores complejos, notaremos como $E^1 \left({\R}\right) = E \left({\R}\right) \cap L^1 \left({\R}\right)$ y a $E^2 \left({\R}\right) = E \left({\R}\right) \cap L^2 \left({\R}\right)$ siendo $L^1\left({\R}\right) \ y \ L^2\left({\R}\right)$ las funciones de m\'odulo absolutamente integrales u en media cuadr\'atica respectivamente, que notemos estos ultimos dos conjuntos son espacios vectoriales de $L^1\ y \ L^2$ con normas:

\[\Vert{f}\Vert_1= \int_{-\infty}^{\infty}{\vert{f(x)}\vert} dx \quad \Vert{f}\Vert_2=\int_{-\infty}^{\infty}{\vert{f(x)}\vert}^2 dx \]
 
Entonces veamos un resultado cl\'asico del an\'alisis real que nos va a permitir intercambiar limites y derivadas con el signo integral (algo que en general quiere):

\begin{teo}\textbf{Teorema de la convergencia Dominada de Lebesgue}\\
Sean $(f_n)\subset L^1(\R), \ g \in L^1(\R) \ y \ f: \ \R \rightarrow \R$ absolutamente integrable sobre intervalos finitos tales que:
\begin{itemize}
\item $f_n(x) \ \rightarrow \ f(x) \ \forall x \in \R $
\item $\vert{f_n(x)}\vert \leq \vert{g(x)}\vert \ \forall x \in \R$
\end{itemize}
Entonces:
\begin{enumerate}
\item $f\in L^1(\R)$
\item $\int_{-\infty}^{\infty}{f_n(x)} dx \ {\rightarrow} \ \int_{-\infty}^{\infty}{f(x)} dx \\ (n \rightarrow \infty) $
\end{enumerate}
\end{teo}

\subsection{Integral de Fourier}
Entonces vimos como las series de Fourier pueden representar una cantidad considerable de funciones peri\'odicas. Intentemos reemplazar la serie por una integral para las funciones no peri\'odicas!!
Sea $f: \ [-l,l] \rightarrow \R \ / \ f,f' \in E([-l,l])$ entonces:
\[ f(x)= \cfrac{a_0}{2} + \sum \limits_{n=1}^{\infty}a_n\cos{\cfrac{n\pi}{l}x} + b_n \sin{\cfrac{n\pi}{l}x}\] 

Donde tenemos 

\[
\left\lbrace \begin{array}{rcl} 
a_n & = & \cfrac{1}{l} \int \limits_{-l}^{l}{f(t)\cos{\left({\cfrac{n\pi}{l}}\right)t}} \ dt \quad n \in \N \\
& & \\
b_n & = & \cfrac{1}{l} \int \limits_{-l}^{l}{f(t)\sin{\left({\cfrac{n\pi}{l}}\right)t}} \ dt \quad n \in \N
\end{array} \right.
\]

Entonces

\[
\begin{array}{rclll}

f(x) & = & \cfrac{1}{2l} \int \limits_{-l}^{l}{f(t)} dt + \cfrac{1}{l} \sum \limits_{k=1}^{\infty}{\int \limits_{-l}^{l}{f(t) \big[ \cos \left({\cfrac{k\pi}{l} t} \right) \cos \left({\cfrac{k\pi}{l} x} \right) + \sin \left({\cfrac{k\pi}{l} t}\right) \sin \left( {\cfrac{k\pi}{l} x} \right) \big] } \ dt} & & \\
& = & \cfrac{1}{2l} \int \limits_{-l}^{l}{f(t)} dt + \cfrac{1}{l} \sum \limits_{k=1}^{\infty}{\int \limits_{-l}^{l}{f(t)\cos\left({\left({\cfrac{n\pi}{l}}\right)x - \left({\cfrac{n\pi}{l}}\right)t}\right)}} \ dt & & \\
& = & \cfrac{1}{2l} \int \limits_{-l}^{l}{f(t)} dt + \cfrac{1}{l} \sum \limits_{k=1}^{\infty}{\int \limits_{-l}^{l}{f(t)\cos\left({\cfrac{k\pi}{l}\left({x-t}\right)}\right)}} \ dt & & \\
& = & \cfrac{1}{2l} \int \limits_{-l}^{l}{f(t)} dt + \cfrac{1}{\pi} \sum \limits_{k=1}^{\infty}{\cfrac{\pi}{l} \int \limits_{-l}^{l}{f(t)\cos\left({k \cfrac{\pi}{l}\left({x-t}\right)}\right)}} \ dt & & \\
& = & \cfrac{1}{2l} \int \limits_{-l}^{l}{f(t)} dt + \cfrac{1}{\pi} \sum \limits_{k=1}^{\infty}{\cfrac{\pi}{l} F_l (k \cfrac{\pi}{l})} \ dt & = & (\bigstar)
\end{array}
\]
\\
si $F_l (y) = \int\limits_{-l}^{l}{f(t)\cos\left({y\left({x-t}\right)}\right)} \ dt$
\\
Con lo cual pensemos en una partici\'on de $\R_{\geq 0}$ dada por $y_0:0 < y_1=\cfrac{\pi}{l}<\dots<y_k=\cfrac{k\pi}{l}< \dots $ con la longitud de cada intervalito $\cfrac{\pi}{l}$ lo que sugiere pensar al segundo sumando de $(\bigstar)$ como una suma de Riemann de la funci\'on $F_l: \ \sum \limits_{k=1}^{\infty}{F_l \left(y_k\right) \Delta y_k} \ dt $. Notemos que la norma de la partici\'on tiende a cero cuando $l$ tiende a infinito. Supongamos ahora que $f \in L^1(\R)$, por lo que la funci\'on $f(t)\cos{y\left(x-t\right)} \in L^1(\R)$, por lo que formalmente podemos decir que \textbf{uniformemente}:

\[F_l(y) \rightarrow F(y)=\int\limits_{-\infty}^{\infty}{f(y)\cos{y(x-t)}} \ dt \ (l \rightarrow \infty)\]

Por lo que podemos decir que claramente $F(y) \in R$ y entonces:

\[\cfrac{1}{\pi} \sum\limits_{k=1}^{\infty}{F_l\left({y_k}\right)\Delta y_k} \ \rightarrow \ \cfrac{1}{\pi}\int\limits_{0}^{\infty}{F\left(y\right)} \ dy \]

Y como se puede ver el primer sumando de $(\bigstar)$ tiende a cero cuando $l$ tiende a infinito debido a que esa integral esta acotada por $\Vert{f}\Vert_1$ (Desigualdad de Bessel un poco modificada pero f\'acil de ver) se puede reescribir que:

\begin{equation}
 f(x)= \cfrac{1}{\pi}\int\limits_{0}^{\infty}{F(y)} \ dy = \cfrac{1}{\pi}\int\limits_{0}^{\infty}{\int\limits_{-\infty}^{\infty}{f(t)\cos\left({y(x-t)}\right)} \ dt } \ dy 
\end{equation}

Y con todos pasos formales logramos pasar de una representaci\'on discreta (la serie) a una continua (la integral). Finalmente para llegar al resultado lindo y compacto, aunque todo el laburo ya lo hicimos, podemos ver que $\cos\left({y(x-t)}\right)$ es par como funci\'on de $y$ y entonces vale que

\[f(x)= \cfrac{1}{2\pi}\int\limits_{-\infty}^{\infty} \left( {\int\limits_{-\infty}^{\infty}{f(t)\cos\left({y(x-t)}\right)} \ dt }\right) \ dy \]

Y si adem\'as suponemos que $g(y)=\int\limits_{-\infty}^{\infty}{f(t)\sin{y(x-t)}} \ dt \in L^1(\R)$ (Como casi siempre pasar\'a e las funciones f\'isicas) tenemos que

\[\cfrac{1}{2\pi}\int\limits_{-\infty}^{\infty} \left( {\int\limits_{-\infty}^{\infty}{f(t)\sin\left({y(x-t)}\right)} \ dt }\right) \ dy = 0\]

Entonces:

\[
\begin{array}{rcl}
f(x) & = & \cfrac{1}{2\pi}\int\limits_{-\infty}^{\infty} \left( {\int\limits_{-\infty}^{\infty}{f(t)\cos\left({y(x-t)}\right)} \ dt }\right) \ dy + i \cfrac{1}{2\pi}\int\limits_{-\infty}^{\infty} \left( {\int\limits_{-\infty}^{\infty}{f(t)\sin\left({y(x-t)}\right)} \ dt }\right) \ dy \\
& = & \cfrac{1}{2\pi}\int\limits_{-\infty}^{\infty} \left( {\int\limits_{-\infty}^{\infty}{f(t)e^{iy(x-t)}} \ dt }\right) \ dy \\
& = & \cfrac{1}{2\pi}\int\limits_{-\infty}^{\infty} \left( {\int \limits_{-\infty}^{\infty}{f(t)e^{-iyt}} \ dt} \right)e^{iyx}  \ dy
\end{array}
\] 

Lo que nos permite y motiva a hacer la siguiente definici\'on:

\begin{defi}
Dada $f \in E^1(\R)$ se le llama \textbf{transformada de Fourier} de $f$ a la funci\'on:
\[
\Fou(f)(\omega)=\widehat{f}(\omega)= \int \limits_{-\infty}^{\infty}{f(t)e^{-i\omega t}} \ dt 
\]
\end{defi}

\subsection{Propiedades de la transformada y algunas transformadas \'utiles}

Ahora que ya definimos y entendimos porqu\'e la transformada de Fourier esta bueno ver algunas propiedades que nos van a simplificar la vida, para luego calcular algunas transformadas en particular y terminar en c\'omo se calculan en general, con herramientas del an\'alisis complejo:

\paragraph{Propiedades}

\begin{enumerate}
\item Si $f \in E^1(E)$, entonces $\widehat{f}$ es continua en $\R$ y $\lim\limits_{\vert{x}\vert \rightarrow \infty}{\widehat{f}(\omega)=0}$ (Esto se demuestra intercambiando l\'imites y integral con el Teo de la convergencia dominada de Lebesgue)
\item Como consecuencia de 1), $\widehat{f}$ es uniformemente continua en $\R \ \forall f \in E^1(\R)$
\item Como dadas $f,g \in E^1(\R) \rightarrow af+bg \in E^1(\R)$ se tiene que $\widehat{af+bg}=a\widehat{f} + b \widehat{g}$
\item Sea $f \in E^1(\R)$ y tal que $\Ima f \subset \R$ entonces
\begin{itemize}
\item $\widehat{f}(-\omega)=\overline{\widehat{f}(\omega)}$
\\
\[\widehat{f}(-\omega)= \int \limits_{-\infty}^{\infty}{f(t)e^{-i(-\omega) t}} \ dt = \int \limits_{-\infty}^{\infty}\overline{{f(t)e^{-i\omega t}}} \ dt = \overline{\int \limits_{-\infty}^{\infty}{f(t)e^{-i\omega t}} \ dt} = \overline{\widehat{f}(\omega)} \]
\item Si adem\'as $f$ es par, entonces $\widehat{f}$ es par y real
\\
\[
\widehat{f}(\omega)= \int \limits_{-\infty}^{\infty}{f(t)e^{-i\omega t}} \ dt = \int \limits_{-\infty}^{\infty}{f(t)\cos{\omega y}} \ dt + i \int \limits_{-\infty}^{\infty}\underbrace{{f(t)\sin{\omega y}}\ dt}_{Impar} = \int \limits_{-\infty}^{\infty}{f(t)\cos{\omega y}} \ dt  \in \R   
\]
\[\widehat{f}(-\omega)=\overline{\widehat{f}(\omega)} = \widehat{f}(\omega)\]
\item Si $f$ es impar, entonces $\widehat{f}$ es impar y puramente imaginaria con una demostraci\'on an\'aloga a la anterior
\end{itemize}
\item Sean $f \in E^1(\R) \ a,b \in \R \ a \neq 0 \ g(x)=f(ax+b)$ entonces

\[\widehat{g}(\omega)= \frac{1}{\vert{a}\vert}e^{\frac{i \omega b}{a}}\widehat{f(\frac{\omega}{a})} \]

En efecto hacer las cuentas con el cambio de variables $u=ax+b$ y sale solo
\item Sea $f \in E^1(R)\ c\in \R$ entonces recordando que $e^xe^y=e^{x+y}$

\[\Fou(e^{-ict}f(t))(\omega)= \widehat{f}(s+c)\] 
\end{enumerate}

Y creo que las dem\'as propiedades sobre las derivadas no ser\'an muy \'utiles.

\paragraph{Tranformadas \'utiles}

Sea $f(x)=e^{-\vert{x}\vert}$ Entonces se ve claramente que $f \in E^1(R)$ entonces por paridad se ve que:

\[\Fou(\omega)=\int\limits_{-\infty}^{\infty}{e^{-\vert{x}\vert}e^{-i\omega x}} \ dx = 2 \int\limits_{0}^{\infty}{e^{-\vert{x}\vert}\cos{\omega x}} \ dx \]

Integremos dos veces por partes y usemos el hecho conocido que $\lim_{x \rightarrow \infty}{e^{-x}}=0$ y obtenemos:

\[\Fou(\omega)= 2 \left[{1-\omega^2 \int\limits_{0}^{\infty}{e^{-\vert{x}\vert}\cos{\omega x}} \ dx  }\right] \]

Lo cual resolviendo para la integral esa dada tenemos:


\[\Fou(\omega) = \cfrac {1}{\pi \left({1+\omega^2}\right)}\]

Por otro lado calcul\'andola tediosamente se puede ver que $\Fou (e^{-x^2})(\omega)= \frac{1}{2\sqrt{\pi}} e^{-\frac{\omega^2}{4}}$

Lo que nos lleva a decir que, usando todas las propiedades dadas:

\begin{equation}
\int_{\infty}^{\infty}{e^{-\left({ax^2+bx}\right)}} \ dx = \sqrt{\frac{\pi}{a}}e^{\frac{b^2}{4a}}
\label{fourier_rel_disp}
\end{equation}

Para finalizar esta secci\'on nos gustar\'ia introducir la forma general del c\'alculo de la transformada de Fourier, para lo cual primero introduciremos un teorema importante

\begin{teo}\textbf{Teorema de Cauchy}
\\
Sea $f: \ \C \rightarrow \C$ entonces si $f$ es derivable en todo punto de un dominio conexo $\Omega$, $f$ es anal\'itica
\end{teo}

\begin{teo}\textbf{Teorema de los Residuos} 
\\
Sea $\gamma$ una curva simple cerrada, y asumamos que $f$ es anal\'itica en $\gamma$ y su interior salvo finitos puntos $z_1,z_2,\dots,z_n$. Entonces se tiene:

\[ \cfrac{1}{2\pi i} \oint_{\gamma}{f(z)} \ dz = \sum\limits_{k=1}^{n}{Res\lbrace{f;z_k}\rbrace}\]
\end{teo}

\begin{teo}\textbf{C\'alculo de Tranformadas de Fourier}
\\
Sea $f$ anal\'itica en $\C$ salvo finitos puntos $z_1,z_2,\dots,z_m$ en el plano superior y $z_{m+1},\dots,z_n$ en el plano inferior. A su vez asumamos $f \in R$ y que $\lim\limits_{R\rightarrow \infty}{\max\limits_{z \in C_R \cup C'_R} \vert{f(z)}\vert}=0$. Entonces:

\[ \Fou(f)(\omega) \left \lbrace \begin{array}{l}
 -i \sum \limits_{k= m+1}^{n}{Res \lbrace{ f\left(z\right) e^{-i \omega z};z_k} \rbrace} \ \ w\geq 0 \\
 \\
 i \sum\limits_{k=1}^{m}{Res \lbrace {f\left(z\right) e^{-i \omega z};z_k} \rbrace} \ \ w\leq 0 
\end{array} \right.
\]
\end{teo}

\end{document}